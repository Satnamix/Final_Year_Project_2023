{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nkashyap-anand/human-posture-estimation/blob/main/human_posture_estimation_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sGsba8RhoccU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TxRKRZ1VsnL2"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.ImageFolder('/content/drive/MyDrive/dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6SdTXevNtP9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a49ebd5c-7586-411b-bb82-0f08590bc8ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['standing', 'bending', 'sitting', 'lying']\n",
            "['standing', 'bending', 'sitting', 'lying']\n"
          ]
        }
      ],
      "source": [
        "data_dir = '/content/drive/MyDrive/dataset'\n",
        "\n",
        "print(os.listdir(data_dir))\n",
        "classes = os.listdir(data_dir)\n",
        "print(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZIgMjYVVusqt"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KmSzPccHut1c"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.ImageFolder('/content/drive/MyDrive/dataset', \n",
        "                transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hqEoXFMwu2BJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36bc52f4-f05b-4c6b-8459-e4aa99387207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 512, 512]) 0\n"
          ]
        }
      ],
      "source": [
        "img_tensor, label = dataset[0]\n",
        "print(img_tensor.shape, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9EHUt56cvbZ7"
      },
      "outputs": [],
      "source": [
        "# random_seed = 42\n",
        "# torch.manual_seed(random_seed)\n",
        "# set the seed of the random number generator to a fixed value\n",
        "# so that when you call for example torch.rand(2), the results will be reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "g1abucmIu8vH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e4427c1-214b-49ac-e1e3-84c6ec68072b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2880, 960, 960)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# 60:20:20 split\n",
        "train_ds, val_ds, test_ds = random_split(dataset, [2880, 960, 960])\n",
        "len(train_ds), len(val_ds), len(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8rDjk_pLvhuP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "batch_size = 16 #start with 64 or 128 and keep doubling it until performance increases "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mNCTgCmFvlPX",
        "outputId": "57289259-35c3-477c-f9d5-49658605eae4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "train_dl = DataLoader(train_ds, \n",
        "                      batch_size,\n",
        "                      shuffle=True,\n",
        "                      num_workers=4,\n",
        "                      pin_memory=True)\n",
        "\n",
        "val_dl = DataLoader(val_ds,\n",
        "                    batch_size,         #can double the batch size because no gradient decent is done here so more memory is free\n",
        "                    num_workers = 4,\n",
        "                    pin_memory=True)\n",
        "\n",
        "\n",
        "# Num_workers tells the data loader instance how many sub-processes to use for data loading. \n",
        "# If the num_worker is zero (default) the GPU has to weight for CPU to load data.\n",
        "# Theoretically, greater the num_workers, more efficiently the CPU load data and less the GPU has to wait.\n",
        "\n",
        "# pin_memory lets your DataLoader allocate the samples in page-locked memory, which speeds-up the transfer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HYoY5pYFv5FN"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "matplotlib.rcParams['figure.facecolor'] = '#ffffff'       \n",
        "#matplotlib.rcParams contains some properties in matplotlibrc file. We can use it to control the defaults of almost every property in Matplotlib: figure size and DPI, line width, color and style, axes, axis and grid properties, text and font properties and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AatFo7nfvtn-"
      },
      "outputs": [],
      "source": [
        "#from torchvision.utils import make_grid\n",
        "\n",
        "# Converts a flow to an RGB image. make_grid (tensor[, nrow, padding, â€¦]) Make a grid of images.\n",
        "\n",
        "# def show_batch(dl):\n",
        "#   for images, labels in dl:\n",
        "#     fig, ax = plt.subplots(figsize=(20,10))\n",
        "#     ax.set_xticks([]); ax.set_yticks([])\n",
        "#     ax.imshow(make_grid(images, nrow=16).permute(1,2,0))\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cIsbGG47wAXI"
      },
      "outputs": [],
      "source": [
        "#show_batch(train_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lbCkYUZJwDj4"
      },
      "outputs": [],
      "source": [
        "#show_batch(val_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUkRogWNxWL-"
      },
      "source": [
        "## Defining the Model (Convolutional Neural Network)\n",
        "\n",
        "we will use a convolutional neural network, using the `nn.Conv2d` class from PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7pDOm-AUxd_a"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hjuxjvYrxibl"
      },
      "outputs": [],
      "source": [
        "conv = nn.Conv2d(3, 8, kernel_size = 3, stride=1, padding=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bOd0k9YrypWd"
      },
      "outputs": [],
      "source": [
        "pool = nn.MaxPool2d(2,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YGb_jWZIysEH"
      },
      "outputs": [],
      "source": [
        "# for images, labels in train_dl:\n",
        "#   print('image.shape: ', images.shape)\n",
        "#   out = conv(images)\n",
        "#   print('out.shape', out.shape)\n",
        "#   out = pool(out)\n",
        "#   print('out.shape after pooling', out.shape)\n",
        "#   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bVJiusPyyw3v"
      },
      "outputs": [],
      "source": [
        "# simple_model = nn.Sequential(\n",
        "#     nn.Conv2d(3,8, kernel_size=3, stride=1, padding=1),\n",
        "#     nn.MaxPool2d(2,2)\n",
        "# )\n",
        "\n",
        "# The objective of nn. Sequential is to quickly implement sequential modules such that \n",
        "# you are not required to write the forward definition, \n",
        "# it being implicitly known because the layers are sequentially called on the outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cjcjJYtgy2ru"
      },
      "outputs": [],
      "source": [
        "# for images, labels in train_dl:\n",
        "#   print('image.shape: ', images.shape)\n",
        "#   out = simple_model(images)\n",
        "#   print('out.shape', out.shape)\n",
        "#   break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj67fi1Wy6WW"
      },
      "source": [
        "\n",
        "## Let's define the model by extending an `ImageClassificationBase` class which contains helper methods for training & validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qGy_5w_My-xl"
      },
      "outputs": [],
      "source": [
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
        "        \n",
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We'll use `nn.Sequential` to chain the layers and activations functions into a single network architecture."
      ],
      "metadata": {
        "id": "er7IKVjBz63Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class human_posture_estimation_CNN(ImageClassificationBase):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            # output: 3 x 512 x 512\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1), \n",
        "            # output: 32 x 512 x 512\n",
        "            nn.ReLU(),\n",
        "            # output: 32 x 512 x 512\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            # output: 64 x 512 x 512\n",
        "            nn.ReLU(),\n",
        "            # output: 64 x 512 x 512\n",
        "            nn.MaxPool2d(2, 2), \n",
        "            # output: 64 x 256 x 256\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 128 x 128 x 128\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 256 x 64 x 64\n",
        "\n",
        "            nn.Flatten(), \n",
        "            nn.Linear(256*64*64, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 4))\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)"
      ],
      "metadata": {
        "id": "uwAPnjSgz7Or"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = human_posture_estimation_CNN()\n",
        "model"
      ],
      "metadata": {
        "id": "8G5oMsrI1fGJ",
        "outputId": "38135d7b-a69a-498d-bcb3-f7ca4a71d769",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "human_posture_estimation_CNN(\n",
              "  (network): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU()\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU()\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU()\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU()\n",
              "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (15): Flatten(start_dim=1, end_dim=-1)\n",
              "    (16): Linear(in_features=1048576, out_features=1024, bias=True)\n",
              "    (17): ReLU()\n",
              "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (19): ReLU()\n",
              "    (20): Linear(in_features=512, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's verify that the model produces the expected output on a batch of training data. The 10 outputs for each image can be interpreted as probabilities for the 10 target classes (after applying softmax), and the class with the highest probability is chosen as the label predicted by the model for the input image. "
      ],
      "metadata": {
        "id": "6_Rv35Tt1td4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for images, labels in train_dl:\n",
        "#   print('image.shape', images.shape)\n",
        "#   out = model(images)\n",
        "#   print('out.shape', out.shape)\n",
        "#   print('out[0]: ', out[0])\n",
        "#   break"
      ],
      "metadata": {
        "id": "Wd-gB2kp1wJg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "metadata": {
        "id": "qpx8HA_k1zMm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = get_default_device()\n",
        "device"
      ],
      "metadata": {
        "id": "ZG2AoomT12OP",
        "outputId": "db6dd78a-461c-47eb-d408-b52064f5ad9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "val_dl = DeviceDataLoader(val_dl, device)\n",
        "to_device(model, device)"
      ],
      "metadata": {
        "id": "N9lotqdI13vX",
        "outputId": "f0353715-b48d-4812-dfc3-944dfce3efe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "human_posture_estimation_CNN(\n",
              "  (network): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU()\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU()\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU()\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU()\n",
              "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (15): Flatten(start_dim=1, end_dim=-1)\n",
              "    (16): Linear(in_features=1048576, out_features=1024, bias=True)\n",
              "    (17): ReLU()\n",
              "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (19): ReLU()\n",
              "    (20): Linear(in_features=512, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model\n",
        "\n",
        "We'll define two functions: `fit` and `evaluate` to train the model using gradient descent and evaluate its performance on the validation set."
      ],
      "metadata": {
        "id": "Kdmn7Lv815mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import optimizer\n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "  model.eval()\n",
        "  outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "  return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
        "  history = []\n",
        "  optimizer = opt_func(model.parameters(), lr)\n",
        "  for epoch in range(epochs):\n",
        "    # traiing phase\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for batch in train_loader:\n",
        "      loss = model.training_step(batch)\n",
        "      train_losses.append(loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "    # validation phases\n",
        "    result = evaluate(model, val_loader)\n",
        "    result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "    model.epoch_end(epoch, result)\n",
        "    history.append(result)\n",
        "  return history"
      ],
      "metadata": {
        "id": "StGouijs18CI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = to_device(human_posture_estimation_CNN(), device)"
      ],
      "metadata": {
        "id": "G4wQvpV-2Cfo"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, val_dl)"
      ],
      "metadata": {
        "id": "_B-B9Vgx2JdU",
        "outputId": "53be12bf-87ae-445e-fdf4-5781d1e0da8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_loss': 1.3864552974700928, 'val_acc': 0.25312501192092896}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "opt_func = torch.optim.Adam\n",
        "lr = 0.001"
      ],
      "metadata": {
        "id": "1oe3ylZO2Pwp"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)"
      ],
      "metadata": {
        "id": "PlBxcini2Tw2",
        "outputId": "3ea0d326-3753-4489-a451-7f7490672bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-5f6b37c59350>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-8f10a1c63b85>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, lr, model, train_loader, val_loader, opt_func)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 14.76 GiB total capacity; 13.53 GiB already allocated; 37.75 MiB free; 13.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_accuracies(history):\n",
        "    accuracies = [x['val_acc'] for x in history]\n",
        "    plt.plot(accuracies, '-x')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.title('Accuracy vs. No. of epochs');"
      ],
      "metadata": {
        "id": "TMZ4L-kc76zF"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(history):\n",
        "    train_losses = [x.get('train_loss') for x in history]\n",
        "    val_losses = [x['val_loss'] for x in history]\n",
        "    plt.plot(train_losses, '-bx')\n",
        "    plt.plot(val_losses, '-rx')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.title('Loss vs. No. of epochs');"
      ],
      "metadata": {
        "id": "2h-my5QB79eg"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(img, model):\n",
        "    # Convert to a batch of 1\n",
        "    xb = to_device(img.unsqueeze(0), device)\n",
        "    # Get predictions from model\n",
        "    yb = model(xb)\n",
        "    # Pick index with highest probability\n",
        "    _, preds  = torch.max(yb, dim=1)\n",
        "    # Retrieve the class label\n",
        "    return dataset.classes[preds[0].item()]"
      ],
      "metadata": {
        "id": "aydIKQ5t7_07"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = test_ds[5]\n",
        "plt.imshow(img.permute(1,2,0))\n",
        "print('Label: ', dataset.classes[label], ', Predicted', predict_image(img, model))"
      ],
      "metadata": {
        "id": "_DJOo01I8Hqf",
        "outputId": "94cf54a6-5680-490b-b0f0-2afcc668855f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label:  sitting , Predicted sitting\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de1QUV54H8G91N/hCAVGERRIPNr7wgaERZjdxB1hMog6ukSiJDybmhKwkMdHdSTKzTsw8NjHOZCfJqslyNBmS40QNWaNRdHyAo0EwQeOIzxAiBpAo8pRXd1fVb/8wVWlsDA10013N73NOH6Xpx6/prm/funXrXoGICIwxZkPn7gIYY56Hg4ExZoeDgTFmh4OBMWaHg4ExZoeDgTFmxyXBsH//fowfPx5GoxHr1q1zxVMwxlxIcPY4BkmSMG7cOBw8eBCjR49GbGwsPvzwQ0yaNMmZT8MYcyGntxg+//xzGI1GREREwNfXF2lpadi1a5ezn4Yx5kIGZz9gVVUVwsPD1Z9Hjx6NEydO/Oh9RowYgTFjxji7FMaYjfLycty4ccOh2zo9GByVlZWFrKwsAMCQIUNQXFzsrlIY6xdMJpPDt3X6rkRYWBgqKirUnysrKxEWFmZ3u4yMDBQXF6O4uBgjR450dhmMsV5wejDExsaitLQUly9fhsViwbZt25CSkuLsp2GMuZDTdyUMBgM2bNiA+++/H5IkYfny5YiKinL20zDGXMglfQyzZ8/G7NmzXfHQjLE+wCMfGWN2OBgYY3Y4GBhjdjgYGGN2OBgYY3Y4GBhjdjgYGGN2OBgYY3Y4GBhjdjgYGGN2OBgYY3Y4GBhjdjgYGGN2OBgYY3Y4GBhjdjgYGGN2OBgYY3Y4GBhjdjgYGGN2OBgYY3Y4GBhjdjgYGGN2OBgYY3Y4GBhjdjgYGGN2OBgYY3Y4GBhjdjgYGGN2OBgYY3Y4GBhjdjgYGGN2OBgYY3Y4GBhjdjgYGGN2ugyG5cuXIzg4GJMnT1avq6urQ3JyMiIjI5GcnIz6+noAABFh5cqVMBqNmDp1Kk6dOuW6yhljLtNlMPz85z/H/v37O1y3bt06JCUlobS0FElJSVi3bh0AYN++fSgtLUVpaSmysrKwYsUK11TNGHOpLoNh5syZGD58eIfrdu3ahfT0dABAeno6PvnkE/X6ZcuWQRAExMfHo6GhAdXV1S4omzHmSj3qY7h27RpCQ0MBACEhIbh27RoAoKqqCuHh4ertRo8ejaqqqk4fIysrCyaTCSaTCTU1NT0pg2kMEXW42F53p///2OVOj8l6z9DbBxAEAYIgdPt+GRkZyMjIAACYTKbelsE0wnYj1uv1kCSpw+fHarUCACRJQlFREdra2kBEGDNmDIxGI3x9fdXb6nQ6yLIMQRBARNDpuC/dWXoUDKNGjUJ1dTVCQ0NRXV2N4OBgAEBYWBgqKirU21VWViIsLMw5lTLNIyI1CIgIoiiisbERTU1NyM3NRVtbG3bt2oW6ujoAQHl5OURRBAAEBgZi5MiRmD17NpKSkhAdHY2goCA1ZHQ6HYioR19SzF6PgiElJQXZ2dl48cUXkZ2djXnz5qnXb9iwAWlpaThx4gT8/f3VXQ7GBEGATqdDU1MTLl68iB07diAvLw9Xr15FQ0ODejtlQ5dlWb1fbW0tamtrcenSJbzzzjsIDQ3FrFmzsGTJEkRHR7vrJXkv6kJaWhqFhISQwWCgsLAw2rx5M924cYMSExPJaDRSUlIS1dbWEhGRLMuUmZlJERERNHnyZPriiy+6engiIoqJiXHodszzybLc4SJJEkmSRLIsU1lZGf3iF78gk8lEvr6+JAgC6XQ60ul0JAiC+rPt9TqdjgwGA+n1+g6/EwSB9Ho9DRs2jN5++20ym81ktVrV55Ikyd1/Co/Tne1MIHJ/j43JZEJxcbG7y2BOQDadgKIoQq/XQxRFnD17Fo899hhKSkqc+nwGgwGDBw/Ga6+9hrS0NPj5+am/0+v1vGthozvbGffWMKdTwkEQBLS3tyMrKwv3338/zp4965Lnam1txerVq7FkyRJcvHiRj1A4AQcD6zXq5HChLMs4duwYli5dil/+8pdqh6IryLIMs9mMv/71r0hMTMTcuXOxc+dONDc3q0c5WPf0+nAlY5IkqYcNJUnC1atXsX37drzxxhu4fv26y59bIcsyampqcODAARw5cgTh4eHIzMzE6tWrXVqDN+IWA+s1Zbfhu+++w9q1axETE4OXXnoJtbW1bqvJYrGgvLwchw4dQmtrq9vq0CpuMbBeEwQBpaWlWLx4Mc6cOQPg1re3Xq93c2VAUVERzp49ixkzZri7FE3hFgPrtevXr+PRRx/FmTNnQESQZVn9152ICA0NDSgoKOh02DW7Mw4G1mvXr1/H+fPn1Z+VDc/dwaAE1ObNm3H58mUOhG7gYGC9tnPnTo8+ken8+fM4duyYu8vQFA4G1m22IUBEuH79unpUwlP97//+L27evOnuMjSDg4H1iCzLkCQJtbW1KCgogCRJkCTJI1sMAPDFF18gNzfX3WVoBgcD6xWz2ayJyXhEUcRbb73FrQYHcTCwHlF2I5qbm9VToz3dyZMnsXfvXneXoQkcDKxHdDoddDod9u7di8bGRneX4xCLxYK33noLzc3N7i7F43EwsB6RJAktLS346KOP3F1KtxQXF2Pz5s1oamry2P4QT8DBwHqEiPDaa6/h9OnT7i6lW0RRxK9+9SskJyd3GHvBOuJgYA6xPTwpyzIKCgqwefNmuzkbtcBsNuPUqVPYtGmT2wdheSoOBuYQ+n6+RkmSkJeXh5///Oeoqanx2EFNP0ap+eOPP8alS5fcXY5H4mBgDpNlGWfOnMHy5ctx9epVdV5GrQUDcOvEr/r6euTk5GiyflfjYGAOEQQBLS0t+K//+i91HRHb32mNLMuQZRklJSUcDJ3gYGAOq66uxt69eyFJEkRRVCdo0eKGpYzcPHLkCCoqKjz6XA934GBg3eYtGw8RoaWlBRaLxWtek7NwMLB+TRRFlJWVqcGgxd0iV+BgYP2axWLB0aNHeTfiNhwMrN9TzgrlcPgBBwPr95RBWp582nhf42Bg/d6+ffv4dOzbcDAwhxARBgwYgMDAQK/roGtpaVFnoOIWwy0cDMwhRITw8HA88sgjXhcM3333HQoKCrzudfUGBwNziCAIEAQBTzzxBCIiItxdjlNZrVYcPHgQRASdjjcJgIOBdQMRYdy4cVizZg18fX2h0+k8YlEZZ9i+fTtKS0vVodL9vSOSg4E5TAmB1NRUPProowBuhYXWw4GIUFdXh+PHj3vEQjmegIOBOUTpmCMi+Pj4IDMzE8OGDVPPsNQyvV4PWZbxzjvvoKKiAsAPi+X011YDBwNziLL/LQgCdDodpk2bhnvuuUezJ1HZUo5IfPnll1i6dCkqKysBdFxJu7/pMhgqKiqQkJCASZMmISoqCm+++SYAoK6uDsnJyYiMjERycjLq6+sB3PoArVy5EkajEVOnTsWpU6dc+wpYn1A6H5VJYAVBQHR0NAwG7a+LrPQryLKM48eP47nnntPkzFTO1GUwGAwGvP766zh//jyKioqwceNGnD9/HuvWrUNSUhJKS0uRlJSEdevWAbg1WKS0tBSlpaXIysrCihUrXP4imOspwaBsLDqdDvfdd5/XfasKgoDDhw/j7Nmz7i7FrboMhtDQUNxzzz0AgKFDh2LixImoqqrCrl27kJ6eDgBIT0/HJ598AgDYtWsXli1bBkEQEB8fj4aGBk0sSMK6x1u/TYkI7e3tuHLlite+Rkd0q4+hvLwcX375JeLi4nDt2jWEhoYCAEJCQtRZfaqqqhAeHq7eZ/To0aiqqnJiycwTyLKMAQMGwGAweNUGpByVUBam8abX1h0OB0NzczMWLFiAN954A8OGDevwO9smpqOysrJgMplgMplQU1PTrfsyzzBjxgyEh4drvvPRltJ/ogSEN7227nAoGKxWKxYsWIDFixfjoYceAgCMGjVK3UWorq5GcHAwACAsLEw95AMAlZWVCAsLs3vMjIwMFBcXo7i4GCNHjuz1C2F9b+DAgfDx8XF3GU6l1+shCAKKiorUDvX+qMtgICI8/vjjmDhxIlavXq1en5KSguzsbABAdnY25s2bp17//vvvg4hQVFQEf39/dZeDeQfbIxSDBg1ydzlOJYoiiAjXr19HW1ubu8txmy6DoaCgAB988AHy8vIQHR2N6Oho5Obm4sUXX8TBgwcRGRmJQ4cO4cUXXwQAzJ49GxERETAajXjiiSewadMml78I1reUgU6CICA1NdWr9sOV1yWKIurq6txdjtt0eRD63nvvveN+1uHDh+2uEwQBGzdu7H1lzKMJggCDwYBBgwZ55enK7e3t2L9/P6ZNm+buUtyCRz6yHlGC4MEHH7TrjNYyQRDUwU5tbW0dpnzztvD7MRwMrEeU3YehQ4d61anKytEIURTx0UcfobW1FaIoav58kO7ynneUuYWPj0+nR528QV1dHaqqqryqD8VRHAysx4gIAQEBiI+Pd3cpLnHt2jWcOXMGAPrdroT2z4BhbqEMBAK8e3SgcljWm19jZzgYWLf1h0Cw1V9epy3elWCM2eFgYKwL/a1/AeBgYE6g1+vVcwy8qdk9dOhQBAcHg4j63eSwHAys15RxDN7WSTdy5EhER0cD6H9zP3IwsB6xHQ2YkJAAvV7vdcEA/DAJrtIi6i84GFi3KU1rSZIgyzJOnToFq9UKq9XqVVO93bx5E2VlZdzHwJijlHCwWq0oLy/3yg2npqYGS5YswaVLl7wq8BzBwcB6TBAEXLhwAbm5ue4uxWUuXryItLQ0fPXVV14ZfnfCwcB6ROlPyM3NRUNDg7vLcRlZlnH+/HmkpaXh448/htVq7RdTvnEwsB4hIjQ2NmLbtm3uLsWllP6F8+fP47HHHsOf/vQn1NTUeH04cDCwHhEEAdu3b8eFCxf6TW+92WzGSy+9hMceewxWq9Xd5bgUBwNzyO0983V1ddi0aVO/6rFX5mr429/+hk2bNqmDnjq7aB2fRMXuSJn/8PbrAODq1asoKytTRzt6w8bQFeUQrdVqxX/+538CADIzMzucgan8vbS+AjgHA+tUZxu6KIoQRRGFhYW4evUqRo0apU5k0h+CwZbZbMbatWvR3NyM5cuXq8snSJKk+VAAOBjYHdi2FJSlAP77v/8bZ86cQXl5OSwWixur8wxtbW347W9/i4MHD+KXv/wlEhIS4OPj4xVT3Wn/FTCnUfahbS/t7e147bXXsGDBAuTk5OCrr77iUEDHv9Vnn32GpUuX4tNPP3V3WU7DLQamUjrOGhsbsXXrVuzcuROtra0oLi7udyP/HGG7+9TY2IjMzEzo9Xp1tTYt42BgKkEQUFdXh2effRY7duyAKIruLkkzBEFAY2MjXnnlFfzzP/8zRowY4e6SeoV3JfqZOx1eIyLcvHkTK1eu5FDoISJCSUkJdu/erfnDlxwM/ZAsyx3OjhRFEfX19XjyySfx4Ycfcij0gCiK6qHMAwcOoLW1Ve2D0CIOhn7I9vCi1WpFVVUVnnnmGXz00Udursw7HDt2DGazGYB2J3jhYOgnbJu0sixDEAS0t7fjD3/4AxITE5GTk8MdjE7S0NCAU6dOdTpATCu487EfsA0FURQhCAIuXryIVatW4ejRo14/7r+vtba2Ij8/HzNnztTsmAZtVs26RTnmLkkSRFHE22+/jfvvvx/5+fncn+ACyuno7e3tvCvBPJeygrMoiti8eTPWrFmDGzduQKfTafaD6+nOnj2Lv/71r+4uo8c4GPoJnU6HiooKrF+/Hi0tLWovOnOdHTt2AOh4iFgrRyk4GPoBpVXw3nvv4fr16+okI9xacK2jR4/i0qVLHQ4Pa6UzkoOhHyAinDhxAh988IFmPphaphwOrq2txTvvvANRFNUjFFoJ4y6Dob29HTNmzMC0adMQFRWFtWvXAgAuX76MuLg4GI1GLFq0SD2xxmw2Y9GiRTAajYiLi0N5eblLXwDrnG3z9bPPPkNaWhqqq6v5CEQfsJ32bceOHbhw4QL0er2mdt26DIYBAwYgLy8Pf//733H69Gns378fRUVFeOGFF7Bq1Sp8/fXXCAwMxJYtWwAAW7ZsQWBgIL7++musWrUKL7zwgstfBOvIdn/27NmzWLJkCSorK71+nkJPouw+1NTUICsrC6IoaurQZZeVCoIAPz8/AFAXFREEAXl5eUhNTQUApKen45NPPgEA7Nq1C+np6QCA1NRUHD58mD+MbiJJEjZt2oTq6mp3l9KvFRQUoLGxUVO7cQ5FmCRJiI6ORnBwMJKTkzF27FgEBATAYLg1Pmr06NGoqqoCAFRVVSE8PBwAYDAY4O/vj9raWrvHzMrKgslkgslkQk1NjbNeD/seESE3Nxc7duzQTE+4tzp37hzOnDmjqffBoWDQ6/U4ffo0Kisr8fnnn+PixYu9fuKMjAwUFxejuLgYI0eO7PXjsY6UVl1jYyO32NyMiLB9+3ZNrQberZ2egIAAJCQkoLCwEA0NDeqoucrKSoSFhQEAwsLCUFFRAeDW8NvGxkYEBQU5uWzWFeUDqKUOL2+l1+tRUVGB9vZ2d5fisC6DoaamRl1pqK2tDQcPHsTEiRORkJCAnJwcAEB2djbmzZsHAEhJSUF2djYAICcnB4mJiZpJScZcgYhQWFiI7777TjOtty5PoqqurkZ6erp67v7ChQsxd+5cTJo0CWlpaVizZg2mT5+Oxx9/HADw+OOPY+nSpTAajRg+fLjXr1TkqXgAk+fQ6XQwm8344osvEBER4e5yHNJlMEydOhVffvml3fURERH4/PPP7a4fOHAgn9fvAZRdCIPBwCdKuZnypZqfn4+HH35YE4ctPb9C1iPKKDtnhoLBYMC0adO4z6iblPciPz9fncDF03EweCmdToeZM2eqh5SdIT09Hfv27VMXV2GOUYZD19XVoaSkxN3lOISDwUtJkoTIyEindfwGBATg6aefxuXLl3HlyhWnPGZ/IggCbt68qY738XQcDF5KEAS8//77Tjs3YsqUKbjrrrtw6NAhtLa2OuUx+wulT2Hs2LGYOXOmm6txDAeDl6qvr8f27dt7/TiCIGDAgAF45plncPPmTbz77rtOqK5/UfoYFi5ciOHDh7u7HIdwMHgpIup1a0FZxXnkyJEwmUxoaGjg8y56aPDgwfjZz36mmTE9HAxeyhkfQGUI77JlyxAeHo7s7Owu163Uyge/ryithaioKM2MYQA4GNiPEAQBgwYNQnJyssO7JhwMHSkDze699174+/trZtAZTx/fS715o125EQmCAB8fn149BhEhISEB8fHxaGpq6nJMhJZmKOpLer0eDz/8sPeeRMU6p8zAbLFYIElSh+nalQsRqf/2xcYzdOhQLFy4sMvb6fV66HQ69aLX69VLUFAQnn76aRgMBrS2tnZ5QpYWRvT1FZ1OB19fX/j6+uLpp59GVFSUu0vqFn4nnUA5z15Z3en48eN49913sXjxYvz5z39GY2Ojuo5hX32j6nQ6LF26FL6+vj96O6UeJSAMBgMiIiIwc+ZMZGVl4ac//SkEQcD+/ftRV1fX5WNp5RvRlQRBUP+eK1euxG9+8xv4+PioXwxawLsSvSTLMsxmMwoLC5Gfn4/c3FycP39enRH4448/xsaNG/HUU0/hZz/7GUaMGNEnG5AgCJgwYQJmzpyJQ4cO3fF2RAQfHx/ExMQgISEB8+fPR2hoqDq60XZNiq4+1LwrcYtyNOeZZ57BSy+9BF9fX+0FJnmAmJgYd5fQLaIokiiKJEkSHTt2jGbPnk1DhgwhnU5HANSLIAjqxcfHh6ZPn05r1qyhs2fPksVi6fA4thdZltXnkmX5jr+7E1mWyWKxkMVioZMnT9I999xDPj4+JAgC6XS6DnXpdDqaNWsWXblyhaxWK8myrD6nLMskiiJZLBZasmRJh9fW2UUQhC5v480XQRBIr9eTr68v/cd//Ac1Nzf/6Hvb17qznQlE7o94k8mE4uJid5fhEPr+xCRJktDW1oYHH3wQJ06ccOi+er0egiAgICAAP/nJTzBlyhTMnDkT0dHR6lR5VqsVPj4+6v667e6HTqdzqAOLvu/jUO7X2NiIEydOwGw2Q5Ik5Ofno6ioCCUlJYiIiMDhw4fVWbR8fHzsHt9qtSImJkYz4/zdQemjISI88sgj2LhxIwYNGqRe7wkthu5sZ7wr0U3K/qOyPuGpU6e6dV9BEFBfX499+/YhNzcXr7/+Ou666y7Ex8cjMzMT48ePx4ABA+zuS993Wjr6AbNt1g8bNgxJSUlqE/df//VfUV9fj927d+O7775DUFCQx3x4tUr5exuNRjz//PMYPHiwehRHi52yHAzdpGxsoigiNze321OnKd/kylL0VqsVZWVl+Oabb7Bnzx5ERETgvvvuU28/ePBgLFiwQP0m9/Pzw1133eVQqwH44ZDo7Rv+8OHDsWzZMuj1+juGjgc0JjWDiDB48GCsXr0aEyZMAHCr9aXVvyEHQw8IgoCGhgYcOHCgWzP/3j4O4PYPTX19PU6ePImTJ092uP4Pf/iDutEGBgbiJz/5CRYvXowZM2Zg1KhRagtGYXuUxHZYtNKsVZ5buZ/tfTt7HJ7opXO3h21SUhLS09M7/J0BbQ764mDohb6aaNV2466trcWePXvw6aefYvTo0XjsscewYsUKDB8+HLIso6WlBc3NzcjLy0N5eTl2794Ns9kMg8GA+++/H7NmzcJ9990HnU4HURRhMBg6beoqoaUcXdHqN5+r2AbqgAEDMHPmTGzYsAGAdxy25WDQGNtdhMrKSvz+979HQUEBnn76abS2tmLTpk24ePGiOm28stsC3Fqa/b333sNf/vIXJCQk3PHDaxsCFosFX331lWZmHupLOp0OISEhWL16NdLT0+Hn56fuImodB0MPEBEqKyvd8m3a2YrJBw8eRF5envp74IfOMNt/ZVlGTU0NNmzYgHvvvddudiclRJqamlBYWIitW7fi22+/RXFxMQcDfmglGAwGyLKMyZMn44MPPoDRaFRbXt7QWgA4GHpElmUcO3YMbW1tbmlmd/Z8t+/WKLe5/V8A6jqWnTl+/DhWrlyJCxcuwGKx8C6EDeWojl6vx8qVK7Fq1Sp1IJijh5K1goOhB5Q3X2mqa40sy9Dr9XbXNzc34/nnn0dJSUmnv+/vlEPVTz31FH79619j0KBBICJNHo7sive9oj6gtBK0tBahrW+++QZnzpxRdy+U17Jz504UFxerk7xoMfRcbe7cuVi7di38/PzUk828pZVgi4OhB5QPglaDoaWlBRs2bFB3Fdra2rBz506sXr0aoij26cleWlNfX6+eN6LsWnjj4DAOhn6IiPB///d/ePvtt9HU1ITf/e53WLFihboUIeucLMs4d+4cWltbvS4Ibsd9DP2Qcnr42rVr8ec//xnnzp1TR0CyH+ctRx26wsHQDykjGZubm9UTo7S6W9TXbHezvDkgeFeCsW6wbVl5cwuLg4GxbjCbzfj222+9OhQADoYesZ0fgfUvkiShubnZ3WW4HH+ye0CSJMTHx2Po0KEcDv3MT3/6U8THx3v9++7dr85FdDodJkyYgMjISK9vUrIf+Pr64tlnn+10Ih1vw8HQQ8OGDUNmZiYPHe5HkpOT8U//9E/Q6XRefxTH4WCQJAnTp0/H3LlzAQCXL19GXFwcjEYjFi1apC5dZjabsWjRIhiNRsTFxaG8vNwlhbuTcphqzpw5SE5Oho+Pjzo81mAwdFibgWmbTqeDj48P7rrrLvzpT3/CgAED1Knh+XAlgDfffBMTJ05Uf37hhRewatUqfP311wgMDMSWLVsAAFu2bEFgYCC+/vprrFq1Ci+88ILzq3YzZRGZ4cOH491338XKlSsxdepUDBkyRA0D5RRcpm3KadZr1qxBRESEegalN4cCAMemj6+oqKDExEQ6fPgwzZkzh2RZpqCgILJarUREdPz4cZo1axYREc2aNYuOHz9ORERWq5WCgoK6nDJbS9PHy7JMVquVrFYrWSwWMpvNJIoiXb9+nY4ePUq//vWvyWg0kl6v7/fTqXvDxcfHhxITE6mxsdGtU787Q3e2M4daDM899xzWr1+v9sTW1taq050DwOjRo1FVVQUAqKqqQnh4OADAYDDA398ftbW1do+ZlZUFk8kEk8mEmpoaR8rwCMqpt8pFmaR1xIgR+Md//Ee89NJLOHLkCP7lX/6lwwk23t6L7a2ICPfddx8GDRrk7lL6VJef1j179iA4OBgxMTFOfeKMjAwUFxejuLhYXdNAK5SmpO1Gb/tvSEgI3nvvPcyePRsDBw6EwWDw/qanFxIEAQMHDsT8+fP73fvXZTAUFBRg9+7dGDNmDNLS0pCXl4dnn30WDQ0N6pj7yspKhIWFAQDCwsJQUVEB4NaY/MbGRgQFBbnwJXgeWZYxYsQIfPDBB/jjH/+IkJAQr+/F9kZEhMTERIwbN87dpfS5LoPh1VdfRWVlJcrLy7Ft2zYkJiZi69atSEhIQE5ODgAgOzsb8+bNAwCkpKQgOzsbAJCTk4PExMR+lbbKZKA6nQ5+fn544okncOTIEfzqV7/CjBkzOl3HsD/9fbRC2Q1csmSJpteH6LHudF7k5+fTnDlziIiorKyMYmNjaezYsZSamkrt7e1ERNTW1kapqak0duxYio2NpbKyMqd2ing6Ze3Hzi5NTU108uRJevHFF+mee+4hf39/0ul0ZDAYyMfHhzssPeRiMBhIr9fThAkTqKqqiiRJIovFQpIkufvj1Su8dqUHIiJ1GXRBENDU1IQLFy7gnXfeQWFhIa5cuaLOH+jIytLMNWxnZVq/fj2eeuopAPCKmZp47UoPZTuJ7NChQxEfHw+TyYS6ujqcOHECH330Ea5cuYKTJ0/CYrFwv4QbKMEwceJEPPLII5oOgt7gYOhDnR2y9PX1RUhICObNm4d58+ahubkZFRUVOHbsGM6dO4f9+/ejubkZDQ0NMJvNHVoSyuPpdDr4+/ujvr5e7eOwXbPSdmp55Wfl20+5/e33U9ZOIJtFa7y1FePj49NhvQ6j0Yjs7GyMGDGi3x5m5mDoI45+8/j5+WH8+DFkG08AAApkSURBVPEYP348RFHEyy+/DFEUcfToUdTV1dmtLSkIAoYMGYLo6Gjk5+dj+/btaG9vx6VLl9Da2tphw1cmGREEAUajEX5+fuoKzQkJCeptlPEn7e3t+N3vfoeysjKvXqZOkiR1TMr06dPx1ltvISoqSv1b98dWA/cxeBjbvgj6fuFZ27fI9vwL229x2/4JURRx5swZtLa2oqKiAnv27EFQUBAWLlwISZJgMBgwZcoUdQi38q3YWeh88803eO2117Bt2za0tbX10V+hbxkMBvzDP/wD1q9fjwceeACDBw/u8DtvCYbubGccDB5GCQTlbbnTStQAOt0NsH07O1vWXrmN7XJqyvPZ9oHYtjDa29uxb98+/Pu//zsqKirU29revjO2td2+S3On29vW6ujtuqrjTo9B3y9d/+CDD2LdunW4++67O+xiKSHcH4OBdyU8THdO0LE9Yaunz2X7b2ePDQBDhgzB/PnzERsbi3/7t3/DoUOH1I5RJWCUEOksNJSWzY/tjtzeEroT2yXm9Xo9rFar+rMsyw512CpBNXjwYLz55ptYsGABhgwZYrdWRH/GwcAcIggCQkNDkZ2djeXLl+Po0aOIi4tDUFAQRowYgdTUVHXjVjb+/Px8XLx4EaWlpfjmm29gtVq7nBatq1C0bVENGjQIvr6+aG9v73bnaEBAAP74xz/i0Ucf7XAmrLe0DnqLg4E5TBAEBAUFYevWraitrUVwcDAGDhwIoGOrRdnI7r33XgiCgNraWjQ2NqKsrAwFBQWdPnZjYyN2796Na9euob29/Y4buU6nQ2hoKBISEpCZmYkBAwZgz549+PTTT/HVV1+hsbHxR1/D8OHDMXnyZPzP//wPJk6caNcysN016c+4j4E5xFkfkzttdKIooqmpCUeOHMG3336LHTt2oKSkBBaLRV1hesqUKXj44YexaNEiBAUFqZPiAEBraytKSkqQn5+PTz/9FFeuXEFAQAACAwMRGBiIhx56CHq9HpMnT0ZkZCT8/Py6XaPWcecj05zbj7DIsoySkhKYzWYAt8Z7REVFdThvQVlQVtmQJUlSH6eyshJDhw6Fv79/hzEdyn2VKQP6E+58ZJpk23Gp1+sRHR3daWeibcdmZ0de9Hq9OieI7e2V/3tri8CZOBiYR7Cdz+JO4zaU293pCIcSKMpRhdsP3cqy3O+PNjiKg4F5BNtv8q6+0e90u9s3+tt/z5PzOo7jkzFmh4OBMWaHg4ExZoeDgTFmh4OBMWaHg4ExZoeDgTFmh4OBMWaHg4ExZoeDgTFmh4OBMWaHg4ExZoeDgTFmh4OBMWaHg4ExZoeDgTFmh4OBMWaHg4ExZoeDgTFmh4OBMWaHg4ExZsehYBgzZgymTJmC6OhomEwmAEBdXR2Sk5MRGRmJ5ORk1NfXA7g1r//KlSthNBoxdepUnDp1ynXVM8ZcwuEWQ35+Pk6fPq2uZLNu3TokJSWhtLQUSUlJWLduHQBg3759KC0tRWlpKbKysrBixQrXVM4Yc5ke70rs2rUL6enpAID09HR88skn6vXLli2DIAiIj49HQ0MDqqurnVMtY6xPOBQMgiBg1qxZiImJQVZWFgDg2rVrCA0NBQCEhITg2rVrAICqqqoOy4ONHj0aVVVVdo+ZlZUFk8kEk8mEmpqaXr8QxpjzOLQS1WeffYawsDBcv34dycnJmDBhQoff92Q9wIyMDGRkZACA2m/BGPMMDrUYwsLCAADBwcGYP38+Pv/8c4waNUrdRaiurkZwcLB624qKCvW+lZWV6v0ZY9rQZTC0tLTg5s2b6v8PHDiAyZMnIyUlBdnZ2QCA7OxszJs3DwCQkpKC999/H0SEoqIi+Pv7q7scjDFt6HJX4tq1a5g/fz4AQBRFPProo3jggQcQGxuLhQsXYsuWLbj77ruxY8cOAMDs2bORm5sLo9GIwYMH47333nPtK2CMOZ1At68l7gZ+fn52/RaeqqamBiNHjnR3GV3SSp2AdmrVSp1A57WWl5fjxo0bDt3foc5HV5swYYI6PsLTmUwmTdSqlToB7dSqlTqB3tfKQ6IZY3Y4GBhjdvQvv/zyy+4uAgBiYmLcXYLDtFKrVuoEtFOrVuoEelerR3Q+MsY8C+9KMMbsuD0Y9u/fj/Hjx8NoNKpnaLrL8uXLERwcjMmTJ6vXeerp5RUVFUhISMCkSZMQFRWFN9980yPrbW9vx4wZMzBt2jRERUVh7dq1AIDLly8jLi4ORqMRixYtgsViAQCYzWYsWrQIRqMRcXFxKC8v75M6FZIkYfr06Zg7d65H1+nyqRDIjURRpIiICCorKyOz2UxTp06lc+fOua2ev/3tb3Ty5EmKiopSr/vFL35Br776KhERvfrqq/T8888TEdHevXvpgQceIFmWqbCwkGbMmNGntV69epVOnjxJRERNTU0UGRlJ586d87h6ZVmmmzdvEhGRxWKhGTNmUGFhIT388MP04YcfEhHRk08+SZs2bSIioo0bN9KTTz5JREQffvghLVy4sE/qVLz++uv0yCOP0Jw5c4iIPLbOu+++m2pqajpc58z33q3BcPz4cZo1a5b68yuvvEKvvPKKGysiunz5codgGDduHF29epWIbm2M48aNIyKijIwM+stf/tLp7dwhJSWFDhw44NH1trS00PTp06moqIiCgoLIarUSUcfPwaxZs+j48eNERGS1WikoKIhkWe6T+ioqKigxMZEOHz5Mc+bMIVmWPbJOos6DwZnvvVt3JRw9Rdudent6eV8oLy/Hl19+ibi4OI+sV5IkREdHIzg4GMnJyRg7diwCAgJgMBjsarGt02AwwN/fH7W1tX1S53PPPYf169dDp7u1WdTW1npknYBrpkKw5REjH7WiJ6eXu1pzczMWLFiAN954A8OGDevwO0+pV6/X4/Tp02hoaMD8+fNx8eJFd5dkZ8+ePQgODkZMTAyOHDni7nK65IqpEGy5tcWghVO0Pfn0cqvVigULFmDx4sV46KGHPL7egIAAJCQkoLCwEA0NDRBF0a4W2zpFUURjYyOCgoJcXltBQQF2796NMWPGIC0tDXl5eXj22Wc9rk6Fq6dCcGswxMbGorS0FJcvX4bFYsG2bduQkpLizpLseOrp5USExx9/HBMnTsTq1as9tt6amho0NDQAANra2nDw4EFMnDgRCQkJyMnJ6bROpf6cnBwkJib2Savn1VdfRWVlJcrLy7Ft2zYkJiZi69atHlcn0EdTITixP6RH9u7dS5GRkRQREUG///3v3VpLWloahYSEkMFgoLCwMNq8eTPduHGDEhMTyWg0UlJSEtXW1hLRrd72zMxMioiIoMmTJ9MXX3zRp7UeO3aMANCUKVNo2rRpNG3aNNq7d6/H1fv3v/+doqOjacqUKRQVFUW/+c1viIiorKyMYmNjaezYsZSamkrt7e1ERNTW1kapqak0duxYio2NpbKysj6p01Z+fr56VMIT6ywrK6OpU6fS1KlTadKkSep248z3nkc+MsbsuH2AE2PM83AwMMbscDAwxuxwMDDG7HAwMMbscDAwxuxwMDDG7HAwMMbs/D/wxJyBNiUovgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DeviceDataLoader(DataLoader(test_ds, batch_size), device)\n",
        "result = evaluate(model, test_loader)\n",
        "result"
      ],
      "metadata": {
        "id": "qXRodzAd92GG",
        "outputId": "337f17a9-c291-4669-a1ca-2f50c7f21301",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_loss': 1.3874242305755615, 'val_acc': 0.23854166269302368}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "human_posture_estimation_CNN",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/Nkashyap-anand/human-posture-estimation/blob/main/human_posture_estimation_CNN.ipynb",
      "authorship_tag": "ABX9TyPguUP/NkKXmpTmPvU+Q+P+",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}